\chapter{Monte Carlo Methods}

Monte Carlo methods are a group of computational methods based on random number generation. In most cases, what we do in Monte Carlo methods is in the following form:
\begin{enumerate}
    \item Generate random numbers;
    \item Solve some problems or compute some values deterministically given the random numbers generated; and
    \item Estimate some quantities, usually by computing weighted averages of the results from deterministic computation.
\end{enumerate}

This chapter will introduce how we can generate random numbers in \julia{} and illustrate the method applied in revenue management and estimating the number of paths in a network.


\section{Probability Distributions}

We use the \href{https://github.com/JuliaStats/Distributions.jl}{\kode{Distributions.jl}}\footnote{\url{https://github.com/JuliaStats/Distributions.jl}} package. Our objective is to generate 100 random numbers from Normal distribution with mean $\mu=50$ and standard deviation $\sigma=7$. First install, if you haven't done yet, the \kode{Distributions.jl} package and import it:
%= lang: julia
\begin{code}
Pkg.add("Distributions")
using Distributions
\end{code}
\noindent Then create a Normal distribution:
%= lang: julia
\begin{code}
d = Normal(50,7)
\end{code}
\noindent Draw 100 random variable from the distribution \kode{d}:
%= lang: julia
\begin{code}
x = rand(d, 100)
\end{code}

Other standard distributions are also available. For example, a discrete distribution, Bernoulli distribution with $p=4$ can be used as follows:
%= lang: julia
\begin{code}
d = Bernoulli(4)
x = rand(d, 100)
\end{code}

Available distributions are listed in the following documents:
\begin{itemize}
\item \href{http://distributionsjl.readthedocs.org/en/latest/univariate-continuous.html}{Univariate Continuous Distributions}\footnote{\url{http://distributionsjl.readthedocs.org/en/latest/univariate-continuous.html}}
\item \href{http://distributionsjl.readthedocs.org/en/latest/univariate-discrete.html}{Univariate Discrete Distributions}\footnote{\url{http://distributionsjl.readthedocs.org/en/latest/univariate-discrete.html}}
\end{itemize}

Suppose now that we have a dataset, and we want to fit a distribution to this dataset and estimate the parameters. For example, we have
%= lang: julia
\begin{code}
data = [2, 3, 2, 1, 4, 2, 1, 1, 2, 1]
\end{code}
\noindent and want to fit with a Binomial distribution. We use the \kode{fit} function:
%= lang: julia
\begin{code}
julia> fit(Geometric, data)
Distributions.Geometric(p=0.3448275862068966)
\end{code}

To create a multivariate Normal distribution, we can use the following code:
%= lang: julia
\begin{code}
mean = [10.0; 20.1]
covariance_matrix = [2.0 1.2; 1.2 3.5]
d = MvNormal(mean, covariance_matrix)
x = rand(d, 5)
\end{code}
\noindent where we generate 5 samples. Note that \kode{covariance\_matrix} is a symmetric matrix. The result is
%= lang: julia
\begin{code}
julia> x = rand(d, 5)
2x5 Array{Float64,2}:
  8.11396   9.0993   8.18729  12.6574   9.0898
 18.5511   21.8543  17.6941   19.8019  17.6679
\end{code}
\noindent We can access the third sample in the third column as follows:
%= lang: julia
\begin{code}
julia> x[:,3]
2-element Array{Float64,1}:
  8.18729
 17.6941
\end{code}









\section{Randomized Linear Program}

In this section, I introduce an application of Monte Carlo methods in revenue management. This section is based on the excellent textbook on the topic by Talluri and van Ryzin (2006)\footnote{Talluri, K.T. and Van Ryzin, G.J., 2006. The Theory and Practice of Revenue Management (Vol. 68). Springer Science \& Business Media.} and the original paper by Talluri and van Ryzin (1999)\footnote{Talluri, K. and Van Ryzin, G., 1999. A Randomized Linear Programming Method for Computing Network Bid Prices. Transportation Science, 33(2), pp.207-216.}.

You don't need to fully understand the context of revenue management is to see how \julia{} can be used to perform tasks in Monte Carlo simulations. I'll just give a short description of a revenue management problem for those who are interested in.

Consider an airline service company that provides several different service products $j=1,...,n$. For each product $j$, the company needs to determine how many tickets to sell, which is denoted by $y_j$. For each product $j$, the demand is a random variable $D_j$. There is a resource constraint
\[
    \mat{A} \vect{y} \leq \vect{x}
\]
where $A_{ij}=1$ if resource $i$ is used by product $j$ and $\vect{x}$ is a vector of available resources. Let us denote the price for each product by $p_j$. Given $\vect{x}$ and the random demand $\vect{D}$, the company considers the following assignment problem:
\[
    H(\vect{x}, \vect{D}) = \max \ \vect{p}^\top \vect{y}
\]
subject to
\begin{align*}
    \mat{A} \vect{y} &\leq \vect{x} \\
    \vect{0} \leq \vect{y} &\leq \vect{D}
\end{align*}
\noindent Note that function $H(\vect{x}, \vect{D})$, which is the optimal objective function value given $\vect{x}, \vect{D}$, is a random varialbe, since $\vect{D}$ is a random variable. Let $\vect{\pi}(\vect{x}, \vect{D})$ denote the dual variable for the resource constraint, which is also a random variable.

In this capacity control problem, what we are interested is to compute:
\[
    \vect{\pi} = \nabla_x \Eb[ H(\vect{x}, \vect{D}) ]
\]
Under some mathematical conditions, the gradient and the expection operators can be interchanged:
\[
\vect{\pi} = \nabla_x \Eb[ H(\vect{x}, \vect{D}) ] = \Eb[ \nabla_x H(\vect{x}, \vect{D}) ] = \Eb[ \pi(\vect{x}, \vect{D})]
\]

To estimate the expected value of dual variables, we use the Monte Carlo simulation. We have the following tasks:
\begin{enumerate}
\item To generate $N$ samples of $\vect{D}$, namely
    \[
        \vect{D}^{(1)}, \vect{D}^{(2)}, ..., \vect{D}^{(N)};
    \]
\item To solve the optimization problem for each $\vect{D}^{(k)}$, $k=1,...,N$, and call the dual variable for the resource constraint $\vect{\pi}^{(k)}$; and
\item To obtain an estimate:
    \[
        \vect{\pi} \approx \frac{1}{N} \sum_{k=1}^N \vect{\pi}^{(k)}.
    \]
\end{enumerate}

If you cannot understand the context of this problem, that's fine; my explanation was short and insufficient. If you are interested in this application, I again refer to the excellent textbook by Talluri and van Ryzin (2006) mentioned earlier. Anyway, we will see how we can complete the above tasks 1, 2, and 3 in \julia{}.





Consider a simple airline network: Tampa (TPA) $\rightarrow$ New York (JFK) $\rightarrow$
Frankfrut (FRA). There are two resources:%
\begin{center}
\begin{tabular}{|c|l|}
\hline
Resource 	& Origin $\rightarrow$ Destination \\ \hline
1 			& TPA $\rightarrow$ JFK \\ \hline
2 			& JFK $\rightarrow$ FRA \\ \hline
\end{tabular}
\end{center}
We have the following six products:%
\begin{center}
\begin{tabular}{|c|l|r|l|}
\hline
Product & Description 					& Revenue 	& Demand $(\mu ,\sigma^2) $	\\ \hline
1 		& TPA to JFK full fare 			& \$150 	& $(30,5^2) $ 				\\ \hline
2 		& TPA to JFK discount 			& \$100 	& $(60,7^2) $ 				\\ \hline
3 		& JFK to FRA full fare 			& \$120 	& $(20,2^2) $ 				\\ \hline
4 		& JFK to FRA discount 			& \$80 		& $(80,4^2) $ 				\\ \hline
5 		& TPA to JFK to FRA full fare 	& \$250 	& $(30,3^2) $ 				\\ \hline
6 		& TPA to JFK to FRA discount 	& \$170 	& $(40,9^2) $ 				\\ \hline
\end{tabular}
\end{center}
Normal distributions $N(\mu ,\sigma^2)$ are assumed for
demand. Suppose the remaining capacity for each resource is given by:%
\begin{center}
\begin{tabular}{|c|l|}
\hline
Resource 	& Remaining Capacity 	\\ \hline
1 			& 120 					\\ \hline
2 			& 230 					\\ \hline
\end{tabular}
\end{center}


To implement the Monte Carlo simulation to estimate $\vect{\pi}$ in \julia{}, let's consider the following basic structure:
\begin{enumerate}
\item Random number generation
%= lang: julia
\begin{code}
samples = Array(Float64, no_products, N)
\end{code}
\noindent We'll \emph{somehow} generate $N$ samples of demand for each product and store it in an array named \kode{samples}.

\item Solving deterministic problems
%= lang: julia
\begin{code}
pi_samples = Array(Float64, no_resources, N)
for k in 1:N
    pi_samples[:,k] = DLP(x, samples[:,k])
end
\end{code}
\noindent We will create a function \kode{DLP} that solves a determinisitc LP problem and returns the optimal dual variable values, which will be stored in an array named \kode{pi\_samples}.

\item Estimating $\vect{\pi}$
%= lang: julia
\begin{code}
pi_estimate = sum(pi_samples,2) / N
\end{code}
\noindent This is a simple way of computing the average. I will explain what the expression \kode{sum(pi\_samples,2)} means later.

\end{enumerate}


First, we import packages that we need:
%= lang: julia
\begin{code}
using JuMP, Gurobi, Distributions
\end{code}
\noindent The above table is converted to arrays in \julia{}: for products
%= lang: julia
\begin{code}
# Product Data
no_products = 6
products = 1:no_products
p = [150; 100; 120; 80; 250; 170]
mu = [30; 60; 20; 80; 30; 40]
sigma = [5; 7; 2; 4; 3; 9]
\end{code}
\noindent for resources
%= lang: julia
\begin{code}
no_resources = 2
resources = 1:no_resources
x = [110; 230]
\end{code}
\noindent The product-resource incident matrix $A$ may be written as:
%= lang: julia
\begin{code}
A = [ 1 1 0 0 1 1 ;
      0 0 1 1 1 1 ]
\end{code}

We design a \julia{} function that accepts $\vect{x}$ and $\vect{D}$ as input and returns the vector of dual variables $\vect{\pi}$ as output:
%= lang: julia
\begin{code}
function DLP(x, D)
    m = Model(solver=GurobiSolver())
    @defVar(m, y[products] >= 0)
    @setObjective(m, Max, sum{ p[j]*y[j], j in products} )

    # Resource Constraints
    @addConstraint(m, rsc_const[i=1:no_resources],
            sum{ A[i,j]*y[i], j in products} <= x[i]  )

    # Upper Bounds
    @addConstraint(m, bounds[j=1:no_products], y[j] <= D[j] )

    solve(m)
    pi = getDual(rsc_const)
    return pi
end
\end{code}
\noindent The above function \kode{DLP} should be self-explaining. We should however note that variables \kode{products}, \kode{p}, \kode{no\_resources}, \kode{no\_products}, and \kode{A} must be accessible within the scope block of function \kode{DLP}; that is, these variables should have been defined within the parent scope block. Please keep this in mind, when you review my complete code later.

Now, we are ready to run the Monte Carlo simulation.
\begin{enumerate}
\item Generate random number for the demand as follows:
%= lang: julia
\begin{code}
N = 100
samples = Array(Float64, no_products, N)
for j in products
    samples[j,:] = rand( Normal(mu[j], sigma[j]), N)
end
\end{code}
\noindent where \kode{samples[j,:]} is the $j$-th row of \kode{samples} and \kode{samples[j,k]} is the $k$-th sample of demand for product $j$.

\item For each sample, solve the optimization problem and obtain the dual variable value:
%= lang: julia
\begin{code}
pi_samples = Array(Float64, no_resources, N)
for k in 1:N
    pi_samples[:,k] = DLP(x, samples[:,k])
end
\end{code}
\noindent where \kode{pi\_samples[:,k]} is the $k$-th column of \kode{pi\_samples} and the element \kode{pi\_samples[i,k]} is the dual variable for resource $i$ with $k$-th demand sample.

\item Compute an estimate of $\vect{\pi}$:
%= lang: julia
\begin{code}
pi_estimate = sum(pi_samples,2) / N
\end{code}
\noindent where \kode{sum(pi\_samples,2)} computes summation over the second dimension of the \kode{pi\_samples} array. Just to see what exactly \kode{sum(pi\_samples,2)} is, observe:
%= lang: julia
\begin{code}
julia> pi_samples
2x100 Array{Float64,2}:
 0.0  37.5  37.5   0.0   0.0    ...    37.5  37.5  37.5  37.5  37.5
 0.0   0.0   0.0  25.0  25.0            0.0  25.0   0.0  25.0   0.0

julia> sum(pi_samples,1)
1x100 Array{Float64,2}:
 0.0  37.5  37.5  25.0  25.0    ...    37.5  62.5  37.5  62.5  37.5

julia> sum(pi_samples,2)
2x1 Array{Float64,2}:
 2550.0
 1425.0
\end{code}


\end{enumerate}


The complete code is shown below:
\begin{codelisting}
\codecaption{Monte Carlo Simulation for Randomized LP \filepath{code/chap7/rlp.jl}}
%= <<(code/chap7/rlp.jl, lang: julia)
\end{codelisting}

The result is
%= lang: julia
\begin{code}
julia> include("rlp.jl")
...
** pi estimate = [26.625 13.5]
\end{code}
\noindent We obtained $\pi_1 = 26.625$ and $\pi_2=13.5$, which are called bid prices. This results suggest that the airline company rejects booking requests for the first resource---flight from TPA to JFK---with the revenue less than \$26.625, and the second resource---flight from JFK to FRA---less than \$13.5. I wish these prices were real.

Since this is a Monte Carlo simulation, the results would change everytime you run the code. With very large $N$, the result would be more accurate.



\section{Estimating the Number of Simple Paths} \label{sec:pathdistribution}

 A path is \emph{simple} if each node in the path is visited only once. In a network, finding the exact number of all available simple paths can take very long time. For small-scale networks, we can simply enumerate all paths.

\begin{figure}
\image{images/montecarlo/simple_network.pdf}
\caption{A simple network with 5 nodes \label{fig:simple_monte}}
\end{figure}

Consider the network in Figure \ref{fig:simple_monte}. There are 5 nodes and 8 undirected links. If we enumerate all simple paths from node 1 to node 5, we obtain the following 9 simple paths:
\begin{align*}
& \{1,2,3,4,5\} \\
& \{1,2,3,5\} \\
& \{1,2,5\} \\
& \{1,3,2,5\} \\
& \{1,3,4,5\} \\
& \{1,3,5\} \\
& \{1,4,3,2,5\} \\
& \{1,4,3,5\} \\
& \{1,4,5\}
\end{align*}
Let $\Pc$ denote the above set of all simple paths and $|\Pc|$ the number of all simple paths. In the above example, $|\Pc|=9$.

One may express the network structure by an node-arc adjacency matrix $\mat{A}$, where $A_{ij}=1$ if there is a link from node $i$ to node $j$. For example, the adjacency matrix of the network in Figure \ref{fig:simple_monte} can be written as follows:
\begin{equation}
    \label{adj1}
    \mat{A} = \begin{bmatrix}
        0 & 1 & 1 & 1 & 0 \\
        1 & 0 & 1 & 0 & 1 \\
        1 & 1 & 1 & 1 & 1 \\
        1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 1 & 0
    \end{bmatrix}
\end{equation}

Then, consider the adjacency matrix for another network:
\begin{equation}
    \label{adj2}
    \mat{A} = \begin{bmatrix}
    0 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 1 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
    1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 1 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\
    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 1 \\
    1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\
    1 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0
    \end{bmatrix}
\end{equation}
There are 16 nodes and 89 directed links. How many simple paths between node 1 to node 16? Well, there are 138,481 simple paths; hence $|\Pc|=138,481$. The number of simple paths increase exponentially. If the size of the network gets bigger and bigger, counting all of simple paths one by one will take very long time. If we need a good estimate for $|\Pc|$, we can use a Monte Carlo method, as suggested by Roberts and Kroese (2007)\footnote{Roberts, B., \& Kroese, D. P. (2007). Estimating the Number of $s$-$t$ Paths in a Graph. Journal of Graph Algorithms and Applications, 11(1), 195--214.}.

Like in other Monte Carlo methods, we will also generate many samples---in this case \emph{sample paths}---for estimating $|\Pc|$. To illustrate this process, let's use the example network in Figure \ref{fig:simple_monte}.

\begin{enumerate}
\item First we begin with the origin node 1. There are three nodes are connected to it: nodes 2, 3, and 4. Randomly choose a node with some probability. Let's use a Uniform distribution. Say, node 4 is chosen with probability 1/3. Our current path is $\{1, 4\}$.
\item Node 4 is connected to nodes 1, 3, and 5. Since node 1 is already visited, we choose between nodes 3 and 5, randomly. Say, node 3 is chosen with probability 1/2. The current path is $\{1, 4, 3\}$.
\item Node 3 is connected to \emph{unvisited} nodes 2 and 5. Again randomly choose between nodes 2 and 5. Say, node 5 is chosen with probability 1/2. The current path is $\{1, 4, 3, 5\}$.
\item Since we arrived at the destination node 5, we stop.
\end{enumerate}

By the above process, one sample path is generated. Let us denote this first sample by $p^{(1)}$. The probability at which this sample path is generated is $g(p^{(1)}) = \frac{1}{3} \times \frac{1}{2} \times \frac{1}{2} = \frac{1}{12}$. We can repeat this process $N$ times to generate $N$ sample paths. In each sample generation, we stop if either the destination node is reached or there is no more unvisited node connected to the current node. We obtain:
\[
\begin{array}{c c c c}
p^{(1)},    &  p^{(2)},   & \ldots,  & p^{(N)}     \\
g(p^{(1)}), & g(p^{(2)}), & \ldots, & g(p^{(N)})
\end{array}
\]


Among the above $N$ samples, there may be some samples that didn't reach the destination node. In the network in Figure \ref{fig:simple_monte}, we will always end up with the destination node, but in general, we don't.

To estimate the number of all simple paths based on the above $N$ samples, we compute the following quantity:
\[
    \widehat{|\Pc|} = \frac{1}{N} \sum_{i}^N \frac{\Ib [ p^{(i)} \text{ reached the destination} ] }{g(p^{(i)})}
\]
where $\Ib[\cdot]$ is a function whose value is 1 if the condition specified in the brackets is true and 0 otherwise. The expression $\Ib [ p^{(i)} \text{ reached the destination} ]$ may also be written as $\Ib[p^{(i)}\in\Pc]$.With sufficiently large $N$, we have $\widehat{|\Pc|} \approx |\Pc|$.

To implement this Monte Carlo method in \julia{}, we first build a function with the following input and output:
\begin{itemize}
\item Input: the adjacency matrix, the origin node, and the destination node
\item Output: whether the path generated reached the destination or not $\Ib[p^{(i)}\in\Pc]$ and the sample's probability $g(p^{(i)})$.
\end{itemize}
That is, we have a function like
%= lang: julia
\begin{code}
function generate_sample_path(adj_mtx, origin, destination)
    # after some calculations

    return  I, g
end
\end{code}
\noindent where \kode{adj\_mtx} stores the adjacency matrix in the form of \eqref{adj1} and \eqref{adj2}.

Let's fill in the function \kode{generate\_sample\_path}. I'll first make a copy of \kode{adj\_mtx}:
%= lang: julia
\begin{code}
adj_copy = copy(adj_mtx)
\end{code}
\noindent I want to make changes in the values of \kode{adj\_mtx} in the process, but still want to preserve the original value of \kode{adj\_mtx}. Note that if you directly change the value of \kode{adj\_mtx} even inside the function \kode{generate\_sample\_path}, you still change the \emph{original} \kode{adj\_mtx}. So I make a copy.

Prepare some initial values
%= lang: julia
\begin{code}
path = [origin]
g = 1
current = origin
\end{code}
\noindent where \kode{path} is an array that I will keep adding node numbers to the end, \kode{g} is the probabiliy value to which I will keep multiflying a probability, and \kode{current} will track at which node we are currently located.

Since the \kode{origin} node is already visited, I'll just disconnect the node from all other nodes in the \emph{copied} adjacency matrix \kode{adj\_copy} as follows:
%= lang: julia
\begin{code}
adj_copy[:, origin] = 0
\end{code}
\noindent which essentially makes the first column of \kode{adj\_copy} zero, so that moving from any other node to the origin node impossible.

Now I'll repeat some computations until we reach the destination node, i.e. \kode{current==destination}. Therefore we prepare a \kode{while} loop:
%= lang: julia
\begin{code}
while current != destination
    # Do some updates on path, g, and current
end
\end{code}
\noindent In the above loop, we will search nodes that are connected to the current node and randomly choose one of them. Note that we don't need to worry if each node is visited or unvisited, since all visited nodes will be discconected in the \kode{adj\_copy} before we search them.

Inside the loop, we will store all nodes that are connected to \kode{current} in an array \kode{V}:
%= lang: julia
\begin{code}
# Inside the loop
V = find(adj_copy[current,:])
\end{code}
\noindent where \kode{find(x)} returns the indices of non-zero elements of array \kode{x}. In this case, we are essentially finding node \kode{i} such that \kode{adj\_copy[current,i] == 1}. Just to make it sure that we understand what \kode{find} does, see the following example:
%= lang: julia
\begin{code}
julia> x = [1 1 0 0 0 1]
julia> find(x)
3-element Array{Int64,1}:
 1
 2
 6
\end{code}

If there is no element in \kode{V}, there is no way to proceed; we stop the loop. The sample path we just generated didn't reach the destination.
%= lang: julia
\begin{code}
# Inside the loop
if length(V)==0
    break
end
\end{code}

Now we choose a node from \kode{V} randomly using a Uniform distribution and add it to \kode{path}:
%= lang: julia
\begin{code}
# Inside the loop
next = rand(V)
path = [path; next]
\end{code}

Before we proceed to the next iteratin of the loop, we update three variables:
%= lang: julia
\begin{code}
# Inside the loop
current = next
adj_copy[:, next] = 0
g = g / length(V)
\end{code}
\noindent The first line updates the current node, the second line disconnects the node we just randomly chose from all other nodes, and the third line update the probability \kode{g}.

The entire \kode{while} loop looks:
%= lang: julia
\begin{code}
while current != destination
    # Find all nodes connected to current
    V = find(adj_copy[current,:])
    if length(V)==0
        break
    end

    # Choose a node randomly and add to path
    next = rand(V)
    path = [path; next]

    # Update variables for the next iteration
    current = next
    adj_copy[:, next] = 0
    g = g / length(V)
end
\end{code}

The above \kode{while} loop will end either because it reached the destination node or because there was no more connected nodes to visit. After the loop, we set the value of \kode{I} either 1 or 0, and return it with the value of \kode{g} as follows:
%= lang: julia
\begin{code}
I = 0
if path[end]==destination
    I = 1
end

return I, g
\end{code}

The complete code is shown below:
\begin{codelisting}
\codecaption{Generating a Sample Path \filepath{code/chap7/generate\_sample\_path.jl}}
%= <<(code/chap7/generate_sample_path.jl, lang: julia)
\end{codelisting}

Calling this function many times, we can generate as many sample paths as we want. Let's test it for the adjacency matrix shown in \eqref{adj1}:
%= lang: julia
\begin{code}
adj_mtx = [
0 1 1 1 0 ;
1 0 1 0 1 ;
1 1 1 1 1 ;
1 0 1 0 1 ;
0 1 1 1 0
]
origin = 1
destination = 5

N = 100
estimate = 0
for i in 1:N
    I, g = generate_sample_path(adj_mtx, origin, destination)
    estimate += I / g
end
estimate = estimate / N

println("estimate = ", estimate)
\end{code}
\noindent In the above code, we generate 100 samples. It's weird to generate 100 paths when there are only 9 paths, but this is a test. The result is:
%= lang: julia
\begin{code}
estimate = 8.85
\end{code}
\noindent which will change everytime we run the code.

For the adjacency matrix shown in \eqref{adj2}, if we generate 1,000 samples, the result looks like:
%= lang: julia
\begin{code}
estimate = 135253.784
\end{code}

This \emph{simple} and \emph{naive} Monte Carlo method for estimating $|\Pc|$ tends to generate shorter paths; hence we need a more advanced sampling technique to generate longer paths more frequently than in the naive sampling, which is called \emph{importance sampling}. Interested readers should read the paper by Roberts and Kroese (2007)\footnote{Roberts, B., \& Kroese, D. P. (2007). Estimating the Number of $s$-$t$ Paths in a Graph. Journal of Graph Algorithms and Applications, 11(1), 195--214.}.

In fact, I have already implemented the algorithm suggested by Roberts and Kroese (2007) and written a package for it. Please check \href{https://github.com/chkwon/PathDistribution.jl}{the \kode{PathDistribution.jl} package}\footnote{\url{https://github.com/chkwon/PathDistribution.jl}}.
