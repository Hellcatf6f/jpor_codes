\chapter{Selected Topics in Numerical Methods}

Although this book does not aim to cover details of numerical methods and algorithms, this chapter will go over very basics of selected topics. Namely, curve fitting, numerical differentiation, and numerical integration are briefly explained. While students and researchers in operations research may not directly use these methods in their own problem solving, the concepts behind these fundamental topics are often useful to understand more complicated and advanced methods. It is also helpful to recognize how related computer software for numerical computations would be designed and what the limitations are.


\section{Curve Fitting}

From data collection or experiment results, we may obtain discrete data sets such as
\[
	(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)
\]
where $x_i$ is an input value and $y_i$ is an output value for each $i=1,...,n$. For example, $x_i$ could be the price of a popular book in time period $i$, and $y_i$ is the corresponding sales volume in time period $i$. Instead of having this discrete data set, we often want to represent the relationship between $x$ and $y$ as an analytical expression, such as a linear function:
\[
	y = \beta_1 + \beta_2 x
\]
or an exponential function
\[
	y = \beta_1 e^{\beta_2 x}
\]
In general, a mathematical function with a vector of parameters $\vect{\beta}$
\[
	y = f(x; \vect{\beta})
\]
that makes sense in the context of $x$, $y$, and the relationship. \emph{Curve fitting} aims to find the values of parameters used in the function form $f(\cdot; \vect{\beta})$ so that the obtained analytical functional form is the closest to the original discrete data set.

Related to curve fitting, \emph{interpolation} finds the values of parameters so that the analytical function \emph{passes through} the discrete data points. This approach assumes that the discrete data set is accurate and exact. When we use polynomial functions for interpolation---called `polynomial interpolation'---available methods are Lagrange's Method, Newton's Method, and Neville's Method.

Instead of using only one polynomial function for the entire data set, we can use a piecewise polynomial function, a function whose segments are separate polynomial functions. The most popular choice is to use a piecewise cubic function, or cubic spline, and the method is called `cubic spline interpolation'. For interpolation methods, most books with `numerical methods' or `numerical analysis' in the title are helpful; see Kiusalaas (2013)\footnote{Kiusalaas, J., 2013. Numerical methods in engineering with Python 3. Cambridge university press.} for example.

In curve fitting, the objective is to determine $\vect{\beta}$ in $f(\cdot; \vect{\beta})$ to match the values of $f(x_i; \vect{\beta})$ to $y_i$ for all $i=1,...,n$ as much as we can. The definition of the best match or best fit depends on one's definition. The most popular definition is based on the least-squares. That is, we aim to minimize
\[
	S(\vect{\beta}) = \sum_{i=1}^n ( y_i - f(x_i; \vect{\beta}) )^2
\]
by optimally choosing $\vect{\beta}$. This is a nonlinear optimization problem in general. When we want a linear function for $f(x)$---called linear regression---the problem becomes a quadratic optimization problem, which can be solved relatively easily.

Finding an optimal $\vect{\beta}$ is related to solving a system of equations:
\[
	\frac{\partial S}{\partial \beta_i} = 0 \quad \forall i=1,...,m
\]
where $m$ is the number of parameters. In case of linear regression, the problem is to solve a system of linear equations.

For general nonlinear least-squares fit, the Levenberg-Marquardt algorithm is popular. See Nocedal and Wright (2006)\footnote{Nocedal, J. and Wright, S., 2006. Numerical optimization. Springer Science \& Business Media.} for details. In \julia{}, \href{https://github.com/JuliaOpt/LsqFit.jl}{the \kode{LsqFit} package}\footnote{\url{https://github.com/JuliaOpt/LsqFit.jl}} from the JuliaOpt group implements the Levenberg-Marquardt algorithm. First add the package:
%= lang:julia
\begin{code}
julia> Pkg.add("LsqFit")
INFO: Cloning cache of LsqFit from git://github.com/JuliaOpt/LsqFit.jl.git
INFO: Installing Distributions v0.8.7
INFO: Installing LsqFit v0.0.2
INFO: Installing PDMats v0.3.6
INFO: Installing StatsBase v0.7.3
INFO: Package database updated
\end{code}
Suppose we have the following data set:
%= lang:julia
\begin{code}
xdata = [ 15.2; 19.9;  2.2; 11.8; 12.1; 18.1; 11.8; 13.4; 11.5;  0.5;
          18.0; 10.2; 10.6; 13.8;  4.6;  3.8; 15.1; 15.1; 11.7;  4.2 ]
ydata = [ 0.73; 0.19; 1.54; 2.08; 0.84; 0.42; 1.77; 0.86; 1.95; 0.27;
          0.39; 1.39; 1.25; 0.76; 1.99; 1.53; 0.86; 0.52; 1.54; 1.05 ]
\end{code}
We would like to use the following function form:
\[
	f(x) = \beta_1 \bigg(\frac{x}{\beta_2}\bigg)^{\beta_3-1} \exp\Bigg( -\bigg(\frac{x}{\beta_2}\bigg)^{\beta_3} \Bigg)
\]
To determine $\vect{\beta}$, we prepare a curve fitting model as a function that returns a vector:
%= lang:julia
\begin{code}
function model(xdata, beta)
	values = similar(xdata)
	for i in 1:length(values)
		values[i] = beta[1] * ((xdata[i]/beta[2])^(beta[3]-1))
                            * (exp( - (xdata[i]/beta[2])^beta[3] ))
	end
	return values
end
\end{code}
\noindent This can be equivalently written as:
%= lang:julia
\begin{code}
model(x,beta) = beta[1] * ((x/beta[2]).^(beta[3]-1)) .*
                          (exp( - (x/beta[2]).^beta[3] ))
\end{code}
\noindent where \kode{.\string^} and \kode{.*} represent element-wise operations.

With some initial guess on $\vect{\beta}$ as \kode{[3.0, 8.0, 3.0]}, we do
%= lang:julia
\begin{code}
using LsqFit
fit = curve_fit(model, xdata, ydata, [3.0, 8.0, 3.0])
\end{code}
\noindent The obtained parameter values are accessed by:
%= lang:julia
\begin{code}
julia> beta = fit.param
3-element Array{Float64,1}:
 1.22516
 0.436552
 0.963007
\end{code}
\noindent and the error estimates for fitting parameters are accessed by:
%= lang:julia
\begin{code}
julia> estimate_errors(fit)
3-element Array{Float64,1}:
 1.02291
 1.16268
 0.331111
\end{code}

\begin{figure}
\image{images/numerical/fit_plot.pdf}
\caption{Curve Fitting Result \label{fig:curve_fit}}
\end{figure}

The result of curve fitting is presented in Figure \ref{fig:curve_fit}. The complete code is provided:
\begin{codelisting}
\codecaption{Curve Fitting \\ \filepath{code/chap4/curve\_fit.jl}}
%= <<(code/chap4/curve_fit.jl, lang: julia)
\end{codelisting}






\section{Numerical Differentiation}

Given a function $f(x)$, we often need to compute its derivative without actually differentiating the function. That is, without the analytical form expressions of $f'(x)$ or $f''(x)$, we need to compute them numerically. This process of \emph{numerical differentiation} is usually done by \emph{finite difference approximations}.

The idea is simple. The definition of the first-order derivative is
\[
	f'(x) = \lim_{h \to \infty} \frac{f(x+h)-f(x)}{h}
\]
from which we obtain a finite difference approximation:
\[
	f'(x) \approx \frac{f(x+h)-f(x)}{h}
\]
for sufficiently small $h>0$. This approximation is called the \emph{forward} finite difference approximation. The \emph{backward} approximation is
\[
	f'(x) \approx \frac{f(x)-f(x-h)}{h}
\]
and the \emph{central} approximation is
\[
	f'(x) \approx \frac{f(x+h)-f(x-h)}{2h}.
\]

Suppose we have discrete points $x_1, x_2, ..., x_n$. For mid-points from $x_2$ to $x_{n-1}$, we can use any finite difference approximation and typically prefer the central approximation. At the boundary points $x_1$ and $x_n$, the central approximation is unavailable; hence we need to use the forward approximation for $x_1$ and the backward approximation for $x_n$.

To be more precise, let us consider the following Taylor series expansions:
\begin{equation}
\label{f1}
f(x+h) = f(x) + hf'(x) + \frac{h^2}{2!}f''(x) + \frac{h^3}{3!}f'''(x) + \frac{h^4}{4!}f^{(4)}(x) + \cdots
\end{equation}
\begin{equation}
\label{f2}
f(x-h) = f(x) - hf'(x) + \frac{h^2}{2!}f''(x) - \frac{h^3}{3!}f'''(x) + \frac{h^4}{4!}f^{(4)}(x) - \cdots
\end{equation}
From \eqref{f1}, we obtain the forward approximation:
\begin{align*}
f'(x) &= \frac{f(x+h)-f(x)}{h} + \frac{h}{2!}f''(x) + \frac{h^2}{3!}f'''(x) + \frac{h^3}{4!}f^{(4)}(x) + \cdots \\
	  &= \frac{f(x+h)-f(x)}{h} + \Oc(h)
\end{align*}
From \eqref{f2}, we obtain the backward approximation:
\[
f'(x) = \frac{f(x)-f(x-h)}{h} + \Oc(h)
\]
By subtracting \eqref{f2} from \eqref{f1}, we obtain the central approximation
\[
f'(x) = \frac{f(x+h)-f(x-h)}{2h} + \Oc(h^2)
\]
Note that the central approximation has the higher-order truncation error $\Oc(h^2)$ than the forward and backward approximations, which means typically smaller errors for sufficiently small $h$.

The second-order derivative is similarly approximated. By adding \eqref{f1} and \eqref{f2}, we obtain
\[
f(x+h) + f(x-h) = 2f(x) + h^2f''(x) + \frac{h^4}{12}f^{(4)}(x) + \cdots
\]
which leads to the central finite difference approximation of the second-order derivative:
\begin{align*}
f''(x) &= \frac{f(x+h)-2f(x)+f(x-h)}{h^2} + \frac{h^2}{12}f^{(4)}(x) + \cdots \\
		&= \frac{f(x+h)-2f(x)+f(x-h)}{h^2} +  \Oc(h^2)
\end{align*}

We can similarly derive the forward and backward approximations of the second-order derivative. We can also approximate the third- and fourth-order derivatives. See Kiusalaas (2013)\footnote{Kiusalaas, J., 2013. Numerical methods in engineering with Python 3. Cambridge university press.} for details.

In \julia{}, the \kode{Calculus.jl} package is available for numerical differentiation. First install and import the package:
%= lang:julia
\begin{code}
Pkg.update()
Pkg.add("Calculus")
using Calculus
\end{code}
Suppose we want to differentiate the following function:
\[
	f(x) = x^3 e^x + \sin x
\]
We prepare the function in \julia{}:
%= lang:julia
\begin{code}
f(x) = x^3 * exp(x) + sin(x)
\end{code}
\noindent The first-order derivative of $f$ at $1.0$:
%= lang:julia
\begin{code}
julia> derivative(f, 1.0)
11.413429620197812
\end{code}
\noindent and the second-order derivative of $f$ at $1.0$:
%= lang:julia
\begin{code}
julia> second_derivative(f, 1.0)
34.49618758929225
\end{code}

For functions with multiple variables, we compute the gradient and hessian. We consider
\[
	g(x) = (x_1)^2 \sin(3x_2) + e^{-2x_3}
\]
\noindent and prepare the function in \julia{}:
%= lang:julia
\begin{code}
g(x) = (x[1])^2 * sin(3x[2]) + exp(-2x[3])
\end{code}
\noindent The gradient at $[3.0, 1.0, 2.0]$:
%= lang:julia
\begin{code}
julia> gradient(g, [3.0, 1.0, 2.0])
3-element Array{Float64,1}:
   0.84672
 -26.7298
  -0.0366313
\end{code}
\noindent and the Hessian at the same point:
%= lang:julia
\begin{code}
julia> hessian(g, [3.0, 1.0, 2.0])
3x3 Array{Float64,2}:
   0.282241  -17.8199  0.0
 -17.8199    -11.4307  0.0
   0.0         0.0     0.0732632
\end{code}

\href{https://github.com/johnmyleswhite/Calculus.jl}{The \kode{Calculus.jl} package}\footnote{\url{https://github.com/johnmyleswhite/Calculus.jl}} also offers symbolic differentiation.






\section{Numerical Integration}

Numerical integration of a continuous function is to approximate
\[
	I = \int_a^b f(x) \dx
\]
The most obvious way of approximating this Riemann integral is using the Riemann sum. With  mesh points $x_1< x_2< ...< x_n$ with $x_1=a$ and $x_n=b$, we may write the Riemann sum as follows:
\[
	R = \sum_{i=1}^{n-1} f(t_i) (x_{i+1}-x_i)
\]
where $t_i\in[x_i,x_{i+1}]$ is an evaluation point in each sub-interval. If $t_i=x_i$, then $R$ is a left Riemann sum, and if $t_1=x_{i+1}$, then $R$ is a right Riemann sum. If $t_i=(x_i+x_{i+1})/2$, then $R$ is a middle Riemann sum.

\begin{figure}
\image{images/numerical/riemann_sum.pdf}
\caption{Riemann Sum\label{fig:riemann}}
\end{figure}

In case of the example shown in Figure \ref{fig:riemann}, the left Riemann sum underestimates the integral, while the right Riemann sum overestimates (but not always). One may think the average of two would be a good approximation. It is called the trapezoidal sum. That is:
\[
\frac{1}{2} \sum_{i=1}^{n-1} f(x_i) (x_{i+1}-x_i) +
\frac{1}{2} \sum_{i=1}^{n-1} f(x_{i+1}) (x_{i+1}-x_i)
= \sum_{i=1}^{n-1} \frac{f(x_i)+f(x_{i+1})}{2}(x_{i+1}-x_i)
\]
When $x_{i+1}-x_i=h$ for all $i$, then
\[
\sum_{i=1}^{n-1} \frac{f(x_i)+f(x_{i+1})}{2} h
= \sum_{i=1}^{n} w_i f(x_i) h
\]
where
\[
w_1 = 1/2,
w_i = 1,
w_n = 1/2
\]
which is the trapezoidal rule for numerical integration.

The trapezoidal rule can be viewed in another perspective. We approximate function $f(x)$ by a straight line in each sub-interval $[x_i, x_{i+1}]$; then the area in the interval looks like a trapezoid. See Figure \ref{fig:trapezoidal}


\begin{figure}
	\image{images/numerical/trapezoidal.pdf}
	\caption{Approximation by Trapezoids \label{fig:trapezoidal}}
\end{figure}


Simpson's rule approximates function $f(x)$ by a quadratic function instead of a straight line. Suppose we have evenly spaced mesh points $a=x_1<x_2<...<x_n=b$. For an arbitrary interval $[x_i, x_{i+2}]$, we use three points---$x_i$, $x_{i+2}$, and the midpoint $x_{i+1}=(x_i+x_{i+2})/2$---to interpolate $f(x)$ by a quadratic function. We approximate
\[
	\int_{x_i}^{x_{i+2}} f(x) \dx \approx \frac{1}{3} \Big[f(x_i) + 4f(x_{i+1}) + f(x_{i+2})\Big] h
\]
For the entire interval we do:
\[
	\int_a^b f(x) \dx = \sum_{i=1,3,5,...}^{n-2} \int_{x_i}^{x_{i+2}} f(x)
\]
which leads to:
\begin{multline}
	\label{simpson1/3}
	\int_a^b f(x) \dx \approx \frac{1}{3}\Big[f(x_1)+4f(x_2)+2f(x_3)+4f(x_4)+...\\
				+2f(x_{n-2})+4f(x_{n-1})+f(x_n) \Big] h \qquad
\end{multline}
We note that $n$ needs to be an odd number, so that the number of sub-intervals, $n-1$, becomes even.

When quadratic functions are used, \eqref{simpson1/3} is called Simpson's 1/3 rule. When a cubic function is used for interpolation in sub-intervals, it leads to Simpson's 3/8 rule. The trapezoidal rule, Simpson's 1/3 rule, and Simpson's 3/8 rule are all of the form:
\[
S = \sum_{i=1}^{n} w_i f(x_i) h
\]
with different rules to define weights $w_i$.

The \kode{quadgk} function provides a method for numerical integration; in particular, Gauss-Kronrod integration method. Suppose we integrate the following function over the interval $[0.0, 1.0]$:
\[
	f(x) = - \cos 3x + x^2 e^{-x}
\]
which is in \julia{}:
%= lang:julia
\begin{code}
f(x) = - cos(3x) + x^2 * exp(-x)
\end{code}
\noindent The integration can be done as follows:
%= lang:julia
\begin{code}
julia> quadgk(f, 0.0, 1.0)
(0.11356279145616598,2.123301534595612e-14)
\end{code}
\noindent where the first output is the integral value and the second output is the error.











\section{Automatic Differentiation}

While numerical differentiation based on finite differences has been popularly used, there is an issue of computational errors from finite differencing. With the developments of modern computer languages, a new paradigm on differentiation using computers has risen: automatic differentiation (AD). We should note that AD computes the derivatives exactly at no significant computational efforts---amazing! Also note that AD is not symbolic differentiation. Some computer programs like \kode{Mathematica} and \kode{Maple}, even \kode{MATLAB} and \julia{}, can do symbolic differentiation, in the same way that humans do differentiation.

To understand how AD works, let us consider ordered pairs in the form of:
\[
	\vec{x} = (x, x'), \qquad \vec{y} = (y, y')
\]
We define operations between these two pairs:
\begin{align*}
	\vec{x} + \vec{y} &= (x, x') + (y, y') = (x+y, \: x'+y') \\
	\vec{x} - \vec{y} &= (x, x') - (y, y') = (x-y, \: x'-y') \\
	\vec{x} \times \vec{y} &= (x, x') \times (y, y') = (xy, \: xy' + x'y) \\
	\frac{\vec{x}}{\vec{y}} &= \frac{(x, x')}{(y, y')} = \bigg( \frac{x}{y}, \: \frac{x'y - xy'}{y^2} \bigg)
\end{align*}
In the third operation for $\times$, look at the second term in the result: $xy'+x'y$, which is exactly the derivative of $xy$. In the fourth operation, the second term represents the derivative of $x/y$. The above operations are defined so that the second term represents the derivative of the first term in the result.

Let's consider an example now. Given a function:
\[
	f(x) = \frac{(x-1)(x+3)}{x}
\]
and we want to compute $f(2)$ and $f'(2)$. (Note, we don't computer $f(x)$ and $f'(x)$; we compute for a specific $x=2$.) We do the following operations:
\begin{align*}
\vec{f}(x) & = \frac{(\vec{x}-\vec{1})(\vec{x}+\vec{3})}{\vec{x}} \\
 &= \frac{ ( (x,1)-(1,0) ) \times ( (x,1)+(3,0) ) }{  (x,1) }
\end{align*}
where we use $\vec{x}=(x,1)$ since $dx/dx=1$ and $\vec{c}=(c,0)$ for any constant $c$. When $x=2$, this leads to:
\begin{align*}
\vec{f}(2) &= \frac{ ( (2,1)-(1,0) ) \times ( (2,1)+(3,0) ) }{  (2,1) }  \\
	&= \frac{ (1,1) \times (5,1) }{ (2,1) }  \\
	&= \frac{ (5,6) }{ (2,1) }  \\
	&= \bigg( \frac{5}{2}, \frac{7}{4}  \bigg)
\end{align*}
Direct computations tell us that $f(2)=5/2$ and $f'(2)=7/4$, which match the results obtained by AD with the ordered pairs.

Using the chain rule, we can define more operations such as:
\begin{align*}
\sin \vec{x} &= \sin(x,x') = (\sin x, x' \cos x) \\
\cos \vec{x} &= \cos(x,x') = (\cos x, - x' \sin x) \\
\exp \vec{x} &= \exp(x,x') = (\exp x, x' \exp x) \\
\log \vec{x} &= \log(x,x') = (\log x, x' / x)
\end{align*}
and the list goes on.

To use AD, the computer needs to know how the function is defined. For example, in \julia{} the above function $f(x)$ is writte as:
%= lang: julia
\begin{code}
function f(x)
	return (x-1) * (x+3) / x
end
\end{code}
\noindent Modern computer languages like \julia{} can recognize each operator like \kode{-}, \kode{+}, \kode{*}, and \kode{/} and substitute each operator with the operation for ordered pairs as defined in AD.

There is \href{http://www.juliadiff.org}{the JuliaDiff group}\footnote{\url{http://www.juliadiff.org}} who implements simple to advanced techniques of AD. The codes from the JuliaDiff group is used in the \kode{Optim} and \jump{} packages.

Just to see how we can use AD in \julia{}, first install the \kode{ForwardDiff.jl} package:
%= lang: julia
\begin{code}
Pkg.add("ForwardDiff")
\end{code}
\noindent With the function definition of $f$, we can compute $f'(2)$ as follows:
%= lang: julia
\begin{code}
using ForwardDiff
g = derivative(f)
g(2)
\end{code}
\noindent For a vector function:
\[
	f(x) = (x_1-2) e^{x_2} - \sin x_3
\]
\noindent we can do as follows:
%= lang: julia
\begin{code}
using ForwardDiff

f(x) = (x[1]-2) * exp(x[2]) - sin(x[3])

g = gradient(f)
h = hessian(f)

g([3.0, 2.0, 1.0])
h([3.0, 2.0, 1.0])
\end{code}
\noindent The result is:
%= lang:julia
\begin{code}
julia> g([3.0, 2.0, 1.0])
3-element Array{Float64,1}:
 -15.2464
  -5.55988
   5.20669

julia> h([3.0, 2.0, 1.0])
3x3 Array{Float64,2}:
   1.93522   36.4478  -33.093
  36.4478    21.3694  -11.9546
 -33.093    -11.9546   13.8836
\end{code}


% \section{Monte Carlo Integration}
%
% Instead of using Riemann-like sums to approximate the area under the curve, we can also approximate the area probabilistically.
%
% \begin{figure}
% \image{images/numerical/monte_carlo_integration.pdf}
% \caption{Monte Carlo Integration \label{fig:monte_carlo_integration}}
% \end{figure}
%
% Consider the curve in Figure \ref{fig:monte_carlo_integration} and the area under the curve. Monte Carlo Integration first considers a clearly larger area whose area is easy to calculate. For example, in the right hand side of Figure \ref{fig:monte_carlo_integration}, we consider a rectangle whose width is $b-a$ and height is higher than the maximum function value in the interval $[a,b]$. Let $R$ denote the area of this rectangle. The area under the curve between the interval $[a,b]$ is denoted by $I$, that is
% \begin{align*}
% 	R &= (b-a) * \text{height} \\
% 	I &= \int_a^b f(x) \dx
% \end{align*}
%
% We first generate pairs of random numbers so that they represent the $(x,y)$-coordinates of \emph{uniformly} distributed points within the rectangle. Then by simple function evaluations, we can tell which point falls under the curve and which doesn't. The integral $I$ can be approximated by:
% \[
% 	I \approx \frac{\text{(number of points that fall under the curve)}}{\text{(number of random points generated)}} R
% \]
% When probability distributions other than Uniform Distribution are used, the above formula, of course, looks different.
