\chapter{General Optimization Problems}

So far, we have learned how we can use the \kode{JuMP.jl} package for solving linear optimization problem with or without integer variables. In this chapter, I will introduce a few more packages that are useful in solving optimization problems.

\section{Unconstrained Optimization}

This package implements basic optimization algorithms, mostly for unconstraint nonlinear optimization problems. It contains many standard algorithms that are found in textbooks for nonlinear or numerical optimization such as Bazaraa et al. (2013)\footnote{Bazaraa, M.S., Sherali, H.D. and Shetty, C.M., 2013. Nonlinear programming: theory and algorithms. John Wiley \& Sons.} and Nocedal and Wright (2006)\footnote{Nocedal, J. and Wright, S., 2006. Numerical optimization. Springer Science \& Business Media.}.

\subsection{Line Search}

Algorithms for optimization problems without constraints or with some simple bounds are probably mostly useful in implementing more advanced algorithms. Often algorithms for constrained problems require solving subproblems, which are line search problems in many cases; e.g. in gradient projection algorithms.

Consider a function $f: \Rb^2 \mapsto \Rb$ in the following form:
\begin{equation}
\label{test_function}
    f(\vect{x}) = (2x_1 - 3)^4 + (3x_1 - x_2)^2
\end{equation}
Let $\bar{\vect{x}}=(2,3)$ and a direction $\bar{\vect{d}}=(-1,0)$. Suppose we are solving the following line search problem:
\begin{equation*}
    \min_{\lambda\in[0,1]} f(\bar{\vect{x}}+\lambda \bar{\vect{d}})
\end{equation*}
which finds an optimal step size from the current point $\bar{\vect{x}}$ in the direction of $\bar{\vect{d}}$.

First define:
%= lang: julia
\begin{code}
x_bar = [2.0, 3.0]
d_bar = [-1.0, 0.0]
\end{code}
We prepare function $f$ in \julia{} as follows:
%= lang: julia
\begin{code}
function f_line(lambda)
    x_new = x_bar + lambda*d_bar
    return (2x_new[1] - 3)^4 + (3x_new[1] - x_new[2])^2
end
\end{code}
\noindent Note that \kode{f\_line} is indeed a function of \kode{lambda}, while \kode{x\_bar} and \kode{d\_bar} are regarded as constant parameters. Inside the function \kode{f\_line}, the values of \kode{x\_bar} and \kode{d\_bar} can be accessed. We need to understand the \emph{scope of variables} in \julia{} clearly to avoid any mistakes. Please read Section \ref{sec:scope} of this book.

Now optimize \kode{f\_line} in the interval \kode{[0.0, 1.0]} using the Golden Section line search algorithm:
%= lang: julia
\begin{code}
using Optim
opt = optimize(f_line, 0.0, 1.0, method=:golden_section)
\end{code}
The optimal $\lambda$ value can be accessed by
%= lang: julia
\begin{code}
julia> opt.minimum
0.8489384490968053
\end{code}
\noindent and the function value at the optimum can be accessed by
%= lang: julia
\begin{code}
julia> opt.f_minimum
0.44257665922759026
\end{code}

The complete code is shown below:
\begin{codelisting}
\codecaption{Line Search \filepath{code/chap6/linesearch.jl}}
%= <<(code/chap6/linesearch.jl, lang: julia)
\end{codelisting}

After the function declaration of \kode{f\_line} and constant parameters \kode{x\_bar} and \kode{d\_bar} in the same script file, it looks natural that the function \kode{f\_line} can access the values of those parameter values. Remember again that understanding the scope of variables, explained in Section \ref{sec:scope}, is important.


\subsection{Unconstrained Optimization}

Consider the function \eqref{test_function} again. This time, we will find a minimum of this function.
We prepare function $f$ in \julia{} as follows:
%= lang: julia
\begin{code}
function f(x)
    return (2x[1] - 3)^4 + (3x[1] - x[2])^2
end
\end{code}
\noindent Optimize this function with an initial point \kode{[1.0, 3.0]}:
%= lang: julia
\begin{code}
using Optim
opt = optimize(f, [1.0, 3.0])
println("optimal x = ", opt.minimum)
println("optimal f = ", opt.f_minimum)
\end{code}
\noindent The result is:
%= lang: julia
\begin{code}
optimal x = [1.4998440884858912,4.499548917284604]
optimal f = 2.772927944725311e-10
\end{code}

While the default algorithm is Nelder-Mead, other algorithms are also available, such as Simulated Annealing, Broyden–Fletcher–Goldfarb–Shanno (BFGS), Conjugate Gradient, etc:
%= lang: julia
\begin{code}
opt = optimize(f, [1.0, 3.0], method=:nelder_mead)
opt = optimize(f, [1.0, 3.0], method=:simulated_annealing)
opt = optimize(f, [1.0, 3.0], method=:bfgs)
opt = optimize(f, [1.0, 3.0], method=:cg)
\end{code}



\subsection{Box-constrained Optimization}
When there are lower and upper bound constraints only, optimization problems are box-constrained. With the function \eqref{test_function}, consider the following problem:
\begin{equation*}
    \min f(\vect{x})
\end{equation*}
subject to
\begin{align*}
    2 \leq x_1 \leq 5 \\
    6 \leq x_2 \leq 10
\end{align*}
We prepare function $f$ in \julia{} as follows:
%= lang: julia
\begin{code}
function f(x)
    return (2x[1] - 3)^4 + (3x[1] - x[2])^2
end
\end{code}
\noindent We also prepare the lower bound, the upper bound, and an initial solution:
%= lang: julia
\begin{code}
lb = [2.0; 6.0]
ub = [5.0; 10.0]
x0 = [3.0; 7.0]
\end{code}
\noindent Then optimize:
%= lang: julia
\begin{code}
fminbox(DifferentiableFunction(f), x0, lb, ub)
\end{code}


For the full list of available algorithms in the \kode{Optim.jl} package, please refer to the \href{https://github.com/JuliaOpt/Optim.jl}{official documentation}.\footnote{\url{https://github.com/JuliaOpt/Optim.jl}}




%\section{Implementing the Gradient Projection Method}
%
%In this section, we'll implement the gradient projection method for solving a class of nonlinear programming problem. Consider
%\[
%\min f(\vect{x})
%\]
%subject to
%\begin{align*}
%\mat{A}\vect{x} &\geq \vect{b} \\
%\vect{x} &\geq \vect{0}
%\end{align*}
%In the above problem, the objective function $f$ is assumed to be a convex nonlinear function and there are linear constraints. We also assume $f$ is differentiable everywhere. Let us define the set of feasible solutions:
%\[
%\Omega = \{ \vect{x} : \mat{A}\vect{x} \geq \vect{b}, \quad \vect{x}\geq \vect{0} \}
%\]
%
%A form of the gradient projection method may be stated as follows:
%\begin{itemize}
%\item \textbf{Step 0:}
%\end{itemize}





\section{Convex Optimization}

For convex optimization problems, I still prefer using \jump{}, given that \cplex{} and \gurobi{} can handle second-order conic programming (SOCP) that involve convex quadratic constraints and objective functions. For example, if you have a problem like:
\begin{equation*}
    \min_{x_1, x_2} (x_1-3)^2 + (x_2-4)^2
\end{equation*}
subject to
\begin{equation*}
    (x_1-1)^2+(x_2+1)^2 \leq 1
\end{equation*}
For this convex optimization problem, we can just use \jump{} as we have done so far. The code below is self-explanatory.
\begin{codelisting}
\codecaption{SOCP with CPLEX \filepath{code/chap6/convex\_jump.jl}}
%= <<(code/chap6/convex_jump.jl, lang: julia)
\end{codelisting}
\noindent The result is
%= lang: julia
\begin{code}
julia> include("convex_jump.jl")
Tried aggregator 1 time.
Aggregator did 1 substitutions.
Reduced QCP has 6 rows, 8 columns, and 12 nonzeros.
Reduced QCP has 2 quadratic constraints.
...
...
...
  11  -5.7703297e+00  -5.7703297e+00  4.47e-06  0.00e+00  3.87e-05  1.32e+07
  12  -5.7703296e+00  -5.7703296e+00  2.89e-08  0.00e+00  2.50e-07  8.44e+07
** Optimal objective function value = 19.229670381903375
** Optimal solution = [1.371435327734289,-0.07154116891275618]
\end{code}

Note that \gurobi{} and \cplex{} can also solve Mixed-Integer SOCP (MISOCP). Hence, in the above code, one can also define the variables as follows:
%= lang: julia
\begin{code}
@defVar(m, x[1:2], Int)
\end{code}
\noindent with the rest of the code being the same.

Another option for convex optimization---without integer variables---would be to use the \kode{Ipopt.jl} package and the underlying \kode{Ipopt} solver\footnote{\url{https://projects.coin-or.org/Ipopt}}, which is an interior point optimizer for large-scale nonlinear optimization problems. First, install the package:
%= lang: julia
\begin{code}
Pkg.add("Ipopt")
\end{code}
\noindent Then, simply change the model declaration part in the above code as follows:
%= lang: julia
\begin{code}
using JuMP, Ipopt
m = Model(solver=IpoptSolver())
\end{code}
\noindent The rest remains the same.

While \jump{} roars as a general purpose modeling language for optimization problems, there is \href{https://github.com/JuliaOpt/Convex.jl}{\kode{Convex.jl}}\footnote{\url{https://github.com/JuliaOpt/Convex.jl}} package, which is based on \href{http://cvxr.com}{\kode{CVX}}\footnote{\url{http://cvxr.com}} in \kode{MATLAB} for convex optimization. Please refer to \href{http://convexjl.readthedocs.org/en/latest/}{the official document}\footnote{\url{http://convexjl.readthedocs.org/en/latest/}} for tutorials and examples.





\section{Nonlinear Optimization}

A general nonlinear optimization problem, which is potentially nonconvex, can also be modeled by \jump{} and solved by \kode{Ipopt}. Consider a nonlinear problem:
\begin{equation*}
    \min_{x_1, x_2} (x_1-3)^3 + (x_2-4)^2
\end{equation*}
subject to
\begin{align*}
    (x_1-1)^2+(x_2+1)^3+e^{-x_1} &\leq 1
\end{align*}
These general nonlinear objective function and constraints are handled by macros \kode{@setNLObjective} and \kode{@addNLConstraint}, respectively. For the above nonlinear optimization problem, we may write:
%= lang: julia
\begin{code}
@setNLObjective(m, Min, (x[1]-3)^3 + (x[2]-4)^2)
@addNLConstraint(m, (x[1]-1)^2 + (x[2]+1)^3 + exp(-x[1]) <= 1)
\end{code}
\noindent See the following complete code:
\begin{codelisting}
\codecaption{NLP with Ipopt \filepath{code/chap6/nlp\_ipopt.jl}}
%= <<(code/chap6/nlp_ipopt.jl, lang: julia)
\end{codelisting}
\noindent The result is
%= lang: julia
\begin{code}
julia> include("nlp_ipopt.jl")
This is Ipopt version 3.12.4, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

...
...
...

EXIT: Optimal Solution Found.
** Optimal objective function value = 4.409110764366554
** Optimal solution = [0.4923986476441378,-0.49188934797281375]
\end{code}


Nonlinear optimization problems may also be solved by algorithms in the \kode{NLopt} library\footnote{\url{http://ab-initio.mit.edu/wiki/index.php/NLopt}}. There are several algorithms available that are callable via the \kode{NLopt.jl} package. Install the package:
%= lang: julia
\begin{code}
Pkg.add("NLopt")
\end{code}
\noindent Then, simply change the model declaration part in the above code as follows:
%= lang: julia
\begin{code}
using JuMP, NLopt
m = Model(solver=NLoptSolver(algorithm=:LD_SLSQP))
\end{code}
\noindent When you use \kode{NLopt}, you need to specify the algorithm. For example \kode{:LD\_SLSQP} means a sequential quadratic programming (SQP) algorithm in the library. Visit \href{http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms}{the wiki page}\footnote{\url{http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms}} of the library for a list of available algorithms in the \kode{NLopt} library. While the original algorithm identifier looks like \kode{NLOPT\_LD\_SLSQP}, the Julia version is \kode{:LD\_SLSQP}, after replacing `\kode{NLOPT\_}' by `\kode{:}'. For example, \kode{NLOPT\_LN\_COBYLA} becomes \kode{:LN\_COBYLA} for the COBYLA (Constrained Optimization BY Linear Approximations) algorithm.


See the following complete code:
\begin{codelisting}
\codecaption{NLP with NLopt \filepath{code/chap6/nlp\_nlopt.jl}}
%= <<(code/chap6/nlp_nlopt.jl, lang: julia)
\end{codelisting}
\noindent The result is
%= lang: julia
\begin{code}
julia> include("nlp_nlopt.jl")
WARNING: Nonlinear solver does not provide dual solutions
** Optimal objective function value = 4.409110147457058
** Optimal solution = [0.49239867641633267,-0.49188921888751513]
\end{code}










\section{Nonconvex Nonlinear Optimization}

With or without integer variables, the \jump{} package can be used to model general nonconvex nonlinear optimization problems. As an example, consider a bi-level optimization problem of the form\footnote{Ex 9.1.1 from Floudas, C.A., Pardalos, P.M., Adjiman, C., Esposito, W.R., Gümüs, Z.H., Harding, S.T., Klepeis, J.L., Meyer, C.A. and Schweiger, C.A., 2013. Handbook of Test Problems in Local and Global Optimization (Vol. 33). Springer Science \& Business Media, \url{http://titan.princeton.edu/TestProblems/}}:
\begin{equation*}
\min -x -3y_1 +2y_2
\end{equation*}
subject to
\begin{align*}
    -2x + y_1 + 4y_2 + s_1 &= 16 \\
    8x + 3y_1 - 2y_2 + s_2 &= 48 \\
    -2x + y_1 - 3y_2 + s_3 &= -12 \\
    -y_1 + s_4 &= 0  \\
    y_1 + s_5 &= 4 \\
    x &\geq 0  \\
    -1 + l_1 + 3l_2 + l_3 - l_4 + l_5 &= 0 \\
    4l_2 - 2l_2 - 3l_3 &= 0 \\
    l_i &\geq 0 \quad i=1,...,5 \\
    s_i &\geq 0 \quad i=1,...,5 \\
    l_i s_i &= 0 \quad i=1,...,5
\end{align*}
The last constraints $l_is_i=0$ are called complementarity conditions, which originate from Karush-Kuhn-Tucker optimality conditions of the original lower-level problem. This class of problems is called a mathematical program with complementarity conditions (MPCC), which is in general a nonconvex, nonlinear optimization problem.

While a problem-specific approach is more desirable, one may attempt to solve this problem by a global optimization solver in \julia{} such as Couenne or Bonmin, both of which are open-source solvers. Couenne stands for Convex Over and Under ENvelopes for Nonlinear Estimation, and solves nonconvex Mixed Integer Nonlinear Programming (MINLP) problems\footnote{\url{https://projects.coin-or.org/Couenne}}. Bonmin stands for Basic Open-source Nonlinear Mixed INteger programming and solves general MINLP problems\footnote{\url{https://projects.coin-or.org/Bonmin}}.

To use these open-source solvers, we need to install packages:
%= lang: julia
\begin{code}
Pkg.add("CoinOptServies")
Pkg.add("AmplNLWriter")
\end{code}
\noindent where the \kode{CoinOptServies.jl} package provides the solver libraries from the COIN-OR projects and the \kode{AmplNLWriter.jl} package provides an interface between \jump{} and the solvers. You may have to restart your \julia{}. Then, call either solver as follows:
%= lang: julia
\begin{code}
using JuMP, AmplNLWriter
m = Model(solver=CouenneNLSolver())
\end{code}
\noindent or
%= lang: julia
\begin{code}
using JuMP, AmplNLWriter
m = Model(solver=BonminNLSolver())
\end{code}

First declare variables:
%= lang: julia
\begin{code}
@defVar(m, x>=0)
@defVar(m, y[1:2])
@defVar(m, s[1:5]>=0)
@defVar(m, l[1:5]>=0)
\end{code}
\noindent Then, set the objective function:
%= lang: julia
\begin{code}
@setObjective(m, Min, -x -3y[1] + 2y[2])
\end{code}
\noindent All linear constraints can be added in the same way as before:
%= lang: julia
\begin{code}
@addConstraint(m, -2x +  y[1] + 4y[2] + s[1] ==  16)
@addConstraint(m,  8x + 3y[1] - 2y[2] + s[2] ==  48)
@addConstraint(m, -2x +  y[1] - 3y[2] + s[3] == -12)
@addConstraint(m,       -y[1]         + s[4] ==   0)
@addConstraint(m,        y[1]         + s[5] ==   4)
@addConstraint(m, -1 + l[1] + 3l[2] +  l[3] - l[4] + l[5] == 0)
@addConstraint(m,     4l[2] - 2l[2] - 3l[3]               == 0)
\end{code}

Now, to add the nonlinear constraints $l_i s_i = 0$, we use \kode{@addNLConstraint}:
%= lang: julia
\begin{code}
for i in 1:5
    @addNLConstraint(m, l[i] * s[i] == 0)
end
\end{code}

After solving the problem by \kode{solve(m)}, we obtain the optimal objective function value as -13.0. The complete code is shown below:
\begin{codelisting}
\codecaption{MPCC example \filepath{code/chap6/ex911.jl}}
%= <<(code/chap6/ex911.jl, lang: julia)
\end{codelisting}

With \kode{Couenne}, the result is:
%= lang: console
{\tiny
\begin{code}
julia> include("ex911.jl")
Couenne 0.5.6 -- an Open-Source solver for Mixed Integer Nonlinear Optimization
Mailing list: couenne@list.coin-or.org
Instructions: http://www.coin-or.org/Couenne
couenne:
ANALYSIS TEST: NLP0012I
              Num      Status      Obj             It       time                 Location
NLP0014I             1         OPT -13       35 0.013441
Couenne: new cutoff value -1.3000000000e+01 (0.019855 seconds)
Loaded instance "/Users/chkwon/.julia/v0.4/AmplNLWriter/.solverdata/tmpIRi1UZ.nl"
Constraints:           12
Variables:             13 (0 integer)
Auxiliaries:            6 (0 integer)

Coin0506I Presolve 3 (-6) rows, 4 (-15) columns and 9 (-20) elements

...
...

** Optimal objective function value = -13.0
** Optimal x = 5.0
** Optimal y = [4.0,2.0]
** Optimal s = [14.0,0.0,0.0,4.0,0.0]
** Optimal l = [0.0,0.1818182896624872,0.12121219310832479,0.0,0.3333329379042136]
\end{code}
}

With \kode{Bonmin}, the result is:
%= lang: console
{\tiny
\begin{code}
julia> include("ex911.jl")
Bonmin 1.8.4 using Cbc 2.9.7 and Ipopt 3.12.4
bonmin:
Cbc3007W No integer variables - nothing to do

******************************************************************************
This program contains Ipopt, a library for large-scale nonlinear optimization.
 Ipopt is released as open source code under the Eclipse Public License (EPL).
         For more information visit http://projects.coin-or.org/Ipopt
******************************************************************************

NLP0012I
              Num      Status      Obj             It       time                 Location
NLP0014I             1         OPT -13       24 0.007729
Cbc3007W No integer variables - nothing to do

 	"Finished"
** Optimal objective function value = -13.0
** Optimal x = 5.0
** Optimal y = [4.0,1.9999999999999998]
** Optimal s = [14.0,7.52316384526264e-37,0.0,4.0,1.8055593228630336e-35]
** Optimal l = [0.0,0.18181828966219965,0.1212121931081331,0.0,0.333332937905268]
\end{code}
}

\section{Mixed Integer Nonlinear Programming}

As discussed in the previous section, both Counenne and Bonmin can handle integer variables. Let all variables in the MPCC example from the previous section to integer variables, and solve again.
%= lang: julia
\begin{code}
@defVar(m, x>=0, Int)
@defVar(m, y[1:2], Int)
@defVar(m, s[1:5]>=0, Int)
@defVar(m, l[1:5]>=0, Int)
\end{code}
\noindent We will obtain the same objective function value, -13.0, with slightly different optimal solutions:
%= lang: console
{\tiny
\begin{code}
Bonmin 1.8.4 using Cbc 2.9.7 and Ipopt 3.12.4
bonmin:

******************************************************************************
This program contains Ipopt, a library for large-scale nonlinear optimization.
 Ipopt is released as open source code under the Eclipse Public License (EPL).
         For more information visit http://projects.coin-or.org/Ipopt
******************************************************************************

NLP0012I
              Num      Status      Obj             It       time                 Location
NLP0014I             1         OPT -13       24 0.008381
NLP0012I
              Num      Status      Obj             It       time                 Location
NLP0014I             1         OPT -13        8 0.003023
NLP0014I             2         OPT -13        0 0
NLP0012I
              Num      Status      Obj             It       time                 Location
NLP0014I             1         OPT -13        0 0
Cbc0012I Integer solution of -13 found by DiveMIPFractional after 0 iterations and 0 nodes (0.00 seconds)
Cbc0001I Search completed - best objective -13, took 0 iterations and 0 nodes (0.00 seconds)
Cbc0035I Maximum depth 0, 0 variables fixed on reduced cost

 	"Finished"
** Optimal objective function value = -13.0
** Optimal x = 5.0
** Optimal y = [4.0,2.0]
** Optimal s = [14.0,0.0,0.0,4.0,0.0]
** Optimal l = [0.0,0.0,0.0,0.0,1.0]
\end{code}
}
\noindent We see that \kode{Bonmin} uses \kode{Ipopt}.
